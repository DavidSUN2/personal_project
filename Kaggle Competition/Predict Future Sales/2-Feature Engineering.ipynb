{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"colab":{"name":"2-Feature Engineering.ipynb","provenance":[],"collapsed_sections":["iQZgCQIuqUqx","eGHBAD8y4Wc4","ChqKjM87qUq-","WZFVLm4RqUrH","JElykmQoqUrM","dlLXO0Lgfo3r","Io6MHmfZqUrg","NN3X-dwGqUrl","WzE4C4LTqUrx","QMwZ3IWpqUr2","ulxS0Pt0qUr8","r5JeGqmjqUsD","cs-597_LqUsZ","2u2B-eqr6WaO","fJBf869_qUsm"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"WHenZ40JqUqv","colab_type":"text"},"source":["# Feature Engineering \n","\n","This file is a list of functions that would generate the final data set based on EDA for models to learn.\n","\n","- preparation\n","- categories and shops cleaning functions\n","- data grouping function\n","- encoding function\n","- grid generation function\n","- adding encodings and additional months, shops and items features\n","- target lagging function\n","- knn features Function\n","- encodings lagging functions\n","- index-based features function\n","- tfidf (pca) features function\n","- final function\n","- save generated data"]},{"cell_type":"markdown","metadata":{"id":"iQZgCQIuqUqx","colab_type":"text"},"source":["### preparation"]},{"cell_type":"markdown","metadata":{"id":"XtASBWLBU9hh","colab_type":"text"},"source":["Connect to google drive, import packages, define helper function and data path variable"]},{"cell_type":"code","metadata":{"id":"7kaiDJLelHfU","colab_type":"code","outputId":"5b75f638-d96c-4051-8807-0f9daa071e42","executionInfo":{"status":"ok","timestamp":1570750296507,"user_tz":360,"elapsed":15835,"user":{"displayName":"DAVID SUN","photoUrl":"","userId":"04535500568104874807"}},"colab":{"base_uri":"https://localhost:8080/","height":125}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2Q4k2_hxlKY7","colab_type":"code","outputId":"acb3705f-643e-4131-ffeb-07ae19de91d2","executionInfo":{"status":"ok","timestamp":1570750297234,"user_tz":360,"elapsed":16548,"user":{"displayName":"DAVID SUN","photoUrl":"","userId":"04535500568104874807"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd '/content/gdrive/My Drive/EY/submission'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Final_Project/DP/submission\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oRtyIG2wqUqy","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd \n","import os\n","import re\n","import time\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.decomposition import PCA\n","from itertools import product\n","import pickle\n","from sklearn.neighbors import NearestNeighbors\n","import scipy.stats"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lu540MTOqUq3","colab_type":"code","colab":{}},"source":["def downcast_dtypes(df):\n","    '''\n","    helper function: downcast dtypes from 64 to 32 bit to save memory \n","    '''\n","    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n","    int_cols =   [c for c in df if df[c].dtype == \"int64\"] \n","    cat_cols =   [c for c in df if df[c].dtype.name == 'category'] \n","    df[float_cols] = df[float_cols].astype(np.float32)\n","    df[int_cols]   = df[int_cols].astype(np.int32)\n","    if len(cat_cols)>0:\n","        df[cat_cols]   = df[cat_cols].astype(np.int32)\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v94Hyp5AqUq7","colab_type":"code","colab":{}},"source":["data_place=os.path.join(os.getcwd(),'input&output/')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eGHBAD8y4Wc4"},"source":["### categories and shops cleaning functions\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"smEWsiz-4Wc5"},"source":["These functions take the groupped data and make following modifications:\n","- clean the shops by merging duplicates and deleting useless shops;\n","\n","- shops_to_change= [[0,1,11],[57,58,10]],\n","shops_to_delete=[8,9,20,23,27,29,30,32,33,40,54];\n","- clean the categories with items that are not in test."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"J-X-XK_L4Wc6","colab":{}},"source":["def shops_clean(data_file,shops_to_change=[[0,1,11],[57,58,10]],shops_to_delete=[8,9,20,23,29,30,32,33,40,54]):\n","    '''\n","    the input is the grouped or initial data_set and 2 lists - shops to merge and shops to delete;\n","    '''\n","    #first merge the duplicate shops by adding the sales of one to the other;\n","    for i in range(len(shops_to_change[0])):\n","        data_file.loc[data_file[\"shop_id\"]==shops_to_change[0][i],'shop_id']=shops_to_change[1][i]\n","    #next delete the useless shops\n","    data_file=data_file[np.logical_not(data_file[\"shop_id\"].isin(shops_to_delete))]\n","    #output is clean data_set\n","    return data_file\n","\n","def cat_clean(data_place,data_file,cat_to_del=[1,  4,  8, 10, 13, 14, 17, 18, 32, 39, 46, 48, 50, 51, 52, 53, 59, 66, 68, 80, 81, 82]):\n","    '''\n","    data place is the path to find items file, data_file is train data, cat_to_del is the list of categories to delete\n","    '''\n","    #read categories\n","    items = pd.read_csv(os.path.join(data_place, 'items.csv'))\n","    #map categories to train data\n","    data_file['cat']=data_file['item_id'].map(pd.Series(items['item_category_id'].values,index=items['item_id'].values))\n","    #next delete the useless categories\n","    data_file=data_file[np.logical_not(data_file['cat'].isin(cat_to_del))]    \n","    return data_file"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ChqKjM87qUq-","colab_type":"text"},"source":["### data grouping function\n"]},{"cell_type":"markdown","metadata":{"id":"kFYhU6pxWWPj","colab_type":"text"},"source":["This function takes original data set as input and make following modifications:\n","- merge by month, item and shop;\n","- target is clipped;\n","- price is clipped by 4 std of given month; \n","- unique shops and items are saved in a way how they appear in train_data - needed for index variables."]},{"cell_type":"code","metadata":{"id":"oCPhsaQ9qUq_","colab_type":"code","colab":{}},"source":["def group_data(data_file, clip=True,save_str=True):\n","    '''\n","    this file groups data per month, item and shop;\n","    data_file is initial file;\n","    clip is variable if to clip prices and sales or not;\n","    save_str is if to save order of shops and items per month; \n","    '''\n","    if save_str==True:\n","        cur_shops=[]\n","        cur_items=[]\n","        for block_num in range(0,data_file[\"date_block_num\"].max()+1):\n","            cur_shops += [data_file[data_file['date_block_num']==block_num]['shop_id'].unique()]\n","            cur_items += [data_file[data_file['date_block_num']==block_num]['item_id'].unique()]\n","    #first set the columns to merge on;\n","    index_cols = ['date_block_num','shop_id','item_id']\n","    #make grouping;\n","    temp=data_file.groupby(index_cols,as_index=False)\n","    #merge the sales and rename them to target;\n","    data_group=temp['item_cnt_day'].sum()\n","    data_group.rename(columns = {'item_cnt_day':'target'}, inplace = True)\n","    #add the max of prices\n","    data_group[\"item_price\"]=temp[\"item_price\"].max()[\"item_price\"]\n","    #next stage is clipping;\n","    if clip==True:\n","        #clipping target for outliers; \n","        data_group['target']=data_group['target'].clip(0,20)\n","        data_group=data_group[data_group['target']!=0]\n","        #prices are more complicated: clip by 4 std of log distribution; \n","        clipped_price=[]\n","        for i in range(0,data_group[\"date_block_num\"].max()+1):\n","            temp=data_group[data_group[\"date_block_num\"]==i]['item_price']\n","            temp2=np.exp(np.log(temp).mean()+4*np.log(temp).std())\n","            if i==0:\n","                clipped_price=data_group[data_group[\"date_block_num\"]==i]['item_price'].clip(0,temp2)\n","            else:\n","                clipped_price=pd.concat([clipped_price,data_group[data_group[\"date_block_num\"]==i]['item_price'].clip(0,temp2)],axis=0)\n","        data_group[\"item_price\"]=clipped_price\n","        \n","    if save_str==True:\n","        return [data_group,[cur_shops,cur_items]]\n","    else:\n","        return data_group"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZFVLm4RqUrH","colab_type":"text"},"source":["### encoding function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2P_hhdFYaL7Q","colab_type":"text"},"source":["This function takes the data file and save encodings of shops, month and items by target and by price.\n","\n","Encoding strategy:\n","The variables to encode:\n","- date;\n","- shop_id;\n","- item_id;\n","\n","There will be 4 types of encoding for 3 categories: \n","- average price per unique item of last month;\n","- average sales per unique item of last month; \n","- number of unique items in last month; \n","- proportion of sales made below max price of given item last month; \n","\n","**NOTE**:\n","Test do not provide price, target and number of unique items (except for general number for a month) data; As a result, all encodings are lagged by 1 month (so here take price and sales of last month);\n","\n","Sales are given per item as the number of unique items change; \n","\n","Number of unique items can be a proxy of size of shop and rank of this parameter can show if shop is big or small; \n","\n","Proportion of items sold below max price can indicate the discounts; use max due to the assumption that shops are seldom allowed to increase price but may be allowed to lower price with discounts. \n","\n","**NOTE 2**:\n","\n","Use log of the encodings as sales and prices; prices seem to be distributed as log norma; sales seem to look more like Poisson distribution but here use log because of the advantages of log which can show the dynamics. \n","   \n","\n","**NOTE 3**:\n","\n","Normalize the encodings by subtracting the general encoding (monthly encoding) from category specific encoding - that will make mean to be closer to zero; Also it will indicate the ranking of shop or item against the general mean; \n","\n","The monthly encodings will be normed by rolling mean of previous periods.\n"]},{"cell_type":"code","metadata":{"id":"8TbneN9dqUrJ","colab_type":"code","colab":{}},"source":["def prepare_encodings(data_file, norm=True):\n","    '''\n","    this function takes the price and sales data per shop, per item and per date for future encodings;\n","    input is grouped data with clipped or not prices and sales;\n","    output is 3 lists to be used later on; \n","    '''\n","    ##########step 0.1 to generate the volume of sales\n","    #classical price index is to take volume of sales and divide by number of sales to get price per unit sold;    \n","    data_file['volume']=data_file[\"item_price\"]*data_file[\"target\"]\n","\n","    #########step 0.5 to find the times when sales was at price below max\n","    #first find max price for each item\n","    temp=data_file.groupby(['item_id',\"date_block_num\"])['item_price'].max().rename('max_price').reset_index()\n","    #then add this variable to general dataset through mapping trick:\n","    #as there is only one combination of item and month, merge then in one index and map needed data with it\n","    #such way is muuuuuch faster then merge; \n","    temp['temp']=temp['item_id']*100+temp[\"date_block_num\"]\n","    data_file['temp']=data_file['item_id']*100+data_file[\"date_block_num\"]\n","    data_file['max_price']=data_file['temp'].map(pd.Series(temp['max_price'].values,index=temp['temp'].values))\n","    #then add low price as proxy of discount\n","    data_file['low_price']=(data_file['item_price']-data_file['max_price'])<0\n","    data_file['low_price_sale']=data_file['target']*data_file['low_price']    \n","    \n","    ##############step 1 to calculate price index and log price change per month (month encoding with price) \n","    #first group by month\n","    temp=data_file.groupby([\"date_block_num\"])\n","    temp2=temp[\"target\"].sum()\n","    #then find price per item: divide volume by number of items sold, name it and reset index;\n","    temp_date_enc=np.log((temp['volume'].sum()/(temp2+0.001)).rename('lprice_index')).reset_index()  # lprice: log price\n","    #next need the number of unique items;\n","    temp_date_enc['uni_index']=temp['item_id'].apply(lambda x: len(x.unique())).values\n","    #next sales per item - total sales divided by number of unique items; \n","    temp_date_enc['ltarget_index']=np.log(temp2.values/temp_date_enc['uni_index']+1)\n","    #proportion of sales with low price to total sales;\n","    temp_date_enc['low_pr_index']=(temp['low_price_sale'].sum().values)/temp2\n","    #now prepare some centrolized features by substractive from feature its cummulative mean from previous dates;\n","    temp_date_enc['lprice_index_norm']=temp_date_enc['lprice_index']-temp_date_enc['lprice_index'].cumsum()/temp_date_enc.date_block_num\n","    temp_date_enc['ltarget_index_norm']=temp_date_enc['ltarget_index']-temp_date_enc['ltarget_index'].cumsum()/temp_date_enc.date_block_num\n","    temp_date_enc['low_pr_index_norm']=temp_date_enc['low_pr_index']-temp_date_enc['low_pr_index'].cumsum()/temp_date_enc.date_block_num\n","    temp_date_enc['uni_index_norm']=np.log(temp_date_enc['uni_index'])-np.log(temp_date_enc['uni_index']).cumsum()/temp_date_enc.date_block_num\n","    #shift the months by 1 - make the lagging;\n","    temp_date_enc[\"date_block_num\"]=temp_date_enc[\"date_block_num\"]+1\n","\n","    ##############step 2 to generate price level of each shop normalized by current price level;\n","    temp=data_file.groupby([\"date_block_num\",\"shop_id\"])\n","    temp2=temp[\"target\"].sum()\n","    #again just divide grouped by shops volume of sales by sales in kind to get price index of shop and log it;\n","    temp_shop_enc=np.log(temp['volume'].sum()/(temp2)).rename('lprice_shop').reset_index()\n","    #next the number of unique items per shop;\n","    temp_shop_enc['uni_shop']=temp['item_id'].apply(lambda x: len(x.unique())).values\n","    #now sales;\n","    temp_shop_enc['ltarget_shop']=np.log(temp2.values/temp_shop_enc['uni_shop']+1)\n","    #proportion of sales with low price;\n","    temp_shop_enc['low_pr_shop']=(temp['low_price_sale'].sum().values)/temp2.values    \n","    #shift the months by 1\n","    temp_shop_enc[\"date_block_num\"]=temp_shop_enc[\"date_block_num\"]+1\n","    #normalizing the encodings;\n","    if norm==True:\n","        temp_shop_enc['lprice_shop']=temp_shop_enc['lprice_shop']-temp_shop_enc['date_block_num'].map(pd.Series(temp_date_enc[\"lprice_index\"].values,index=temp_date_enc[\"date_block_num\"].values))\n","        temp_shop_enc['ltarget_shop']=temp_shop_enc['ltarget_shop']-temp_shop_enc['date_block_num'].map(pd.Series(temp_date_enc['ltarget_index'].values,index=temp_date_enc[\"date_block_num\"].values))\n","        temp_shop_enc['low_pr_shop']=temp_shop_enc['low_pr_shop']-temp_shop_enc['date_block_num'].map(pd.Series(temp_date_enc['low_pr_index'].values,index=temp_date_enc[\"date_block_num\"].values))\n","        temp_shop_enc['uni_shop']=temp_shop_enc['uni_shop']/(temp_shop_enc['date_block_num'].map(pd.Series(temp_date_enc['uni_index'].values,index=temp_date_enc[\"date_block_num\"].values))+0.001)\n","    \n","    ##############step 3 do alike with items\n","    temp=data_file.groupby([\"date_block_num\",\"item_id\"])\n","    temp2=temp[\"target\"].sum()\n","    #as with shops\n","    temp_item_enc=np.log(temp['volume'].sum()/(temp2)).rename('lprice_item').reset_index()\n","    #obviously for items not need number of unique items;\n","    temp_item_enc['ltarget_item']=np.log(temp2+1).values\n","    #proportion of sales with low price;\n","    temp_item_enc['low_pr_item']=(temp['low_price_sale'].sum().values)/temp2.values   \n","    temp_item_enc[\"date_block_num\"]=temp_item_enc[\"date_block_num\"]+1\n","    if norm==True:\n","        temp_item_enc['lprice_item']=temp_item_enc['lprice_item']-temp_item_enc['date_block_num'].map(pd.Series(temp_date_enc[\"lprice_index\"].values,index=temp_date_enc[\"date_block_num\"].values))\n","        temp_item_enc['ltarget_item']=temp_item_enc['ltarget_item']-temp_item_enc['date_block_num'].map(pd.Series(temp_date_enc['ltarget_index'].values,index=temp_date_enc[\"date_block_num\"].values))\n","        temp_item_enc['low_pr_item']=temp_item_enc['low_pr_item']-temp_item_enc['date_block_num'].map(pd.Series(temp_date_enc['low_pr_index'].values,index=temp_date_enc[\"date_block_num\"].values))\n","    \n","    return (temp_date_enc,temp_shop_enc,temp_item_enc)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JElykmQoqUrM","colab_type":"text"},"source":["### grid generation function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"sMzf_BkGeDW0","colab_type":"text"},"source":["This function takes the true distribution of shops and items presented at given month to create grid of all items and shops combinations, add sales if they happened or add 0; \n","\n","After this function stage the test data can be added so that future features are generated in the same loops."]},{"cell_type":"code","metadata":{"id":"6l6awg2eqUrN","colab_type":"code","colab":{}},"source":["def grid_gen(df,temp_order=None,save_str=True):\n","    '''\n","    input is the grouped data and saved order of unique shops and items; \n","    '''\n","    #first the columns to merge on\n","    index_cols = ['shop_id', 'item_id', 'date_block_num']\n","    #then make a grid as combination of all active shops and active items of given month;\n","    grid = [] \n","    for block_num in range(0,df[\"date_block_num\"].max()+1):\n","        #in case care about order\n","        if save_str==True:\n","            cur_shops = temp_order[0][block_num]\n","            cur_items = temp_order[1][block_num]\n","        #in case not care\n","        else:\n","            cur_shops = df[df['date_block_num']==block_num]['shop_id'].unique()\n","            cur_items = df[df['date_block_num']==block_num]['item_id'].unique()\n","        grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n","    #convert grid to DataFrame\n","    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n","    #finally make the mapping trick to pass the sales values to grid\n","    grid['temp']=grid['item_id']*10000+grid['shop_id']*100+grid['date_block_num']\n","    df['temp']=df['item_id']*10000+df['shop_id']*100+df['date_block_num']\n","    grid['target']=grid[\"temp\"].map(pd.Series(df[\"target\"].values,index=df[\"temp\"].values)).fillna(0)\n","    #drop temp\n","    grid=grid.drop(columns=[\"temp\"])\n","    return grid\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dlLXO0Lgfo3r"},"source":["### adding encodings and additional months, shops and items features\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SokUi-uyfqdr","colab_type":"text"},"source":["In this stage:\n","- add encodings and new features of month:\n"," - 3 features that describe the month: month, long month dummy and December dummy and unique items of current month;\n","- add encodings and new features of shops: \n"," - extract city name and encode it alike shops; add shops encodings to grid; \n","- add encodings and new features of items:\n"," - add categories of items and encode them;\n"," - add additional features for categories: major categories and several dummies;\n"," - add date of first sale and dummy is this is new item;\n"," - add item encodings."]},{"cell_type":"code","metadata":{"id":"jSd-7H29qUrS","colab_type":"code","colab":{}},"source":["def add_gen_feat(df,temp_date_enc,norm=True):\n","    '''\n","    input is grid, saved encodings and bool either to norm data; \n","    '''\n","    df[\"lprice_index\"]=df[\"date_block_num\"].map(pd.Series(temp_date_enc[\"lprice_index\"].values,index=temp_date_enc[\"date_block_num\"].values))\n","    df[\"ltarget_index\"]=df[\"date_block_num\"].map(pd.Series(temp_date_enc[\"ltarget_index\"].values,index=temp_date_enc[\"date_block_num\"].values))\n","    df[\"uni_index\"]=df[\"date_block_num\"].map(pd.Series(temp_date_enc[\"uni_index\"].values,index=temp_date_enc[\"date_block_num\"].values))\n","    df[\"low_pr_index\"]=df[\"date_block_num\"].map(pd.Series(temp_date_enc[\"low_pr_index\"].values,index=temp_date_enc[\"date_block_num\"].values))\n","    #fill na with 0 as NAs should be only at first month that will anyway be cut by lagging; \n","    df[\"lprice_index\"].fillna(0,inplace=True)\n","    df[\"ltarget_index\"].fillna(0,inplace=True)\n","    df[\"uni_index\"].fillna(0,inplace=True)\n","    df[\"low_pr_index\"].fillna(0,inplace=True)\n","    #add normalized variables - 3 more variables usefull for linear models;\n","    if norm==True:\n","        df[\"lprice_index_norm\"]=df[\"date_block_num\"].map(pd.Series(temp_date_enc[\"lprice_index_norm\"].values,index=temp_date_enc[\"date_block_num\"].values))\n","        df[\"ltarget_index_norm\"]=df[\"date_block_num\"].map(pd.Series(temp_date_enc[\"ltarget_index_norm\"].values,index=temp_date_enc[\"date_block_num\"].values))\n","        df[\"low_pr_index_norm\"]=df[\"date_block_num\"].map(pd.Series(temp_date_enc[\"low_pr_index_norm\"].values,index=temp_date_enc[\"date_block_num\"].values))\n","        df['uni_index_norm']=df[\"date_block_num\"].map(pd.Series(temp_date_enc['uni_index_norm'].values,index=temp_date_enc[\"date_block_num\"].values))\n","        df[\"lprice_index_norm\"].fillna(0,inplace=True)\n","        df[\"ltarget_index_norm\"].fillna(0,inplace=True)\n","        df[\"low_pr_index_norm\"].fillna(0,inplace=True)\n","        df['uni_index_norm'].fillna(0,inplace=True)\n","    #here add categories of month;\n","    df[\"month_cat\"]=df[\"date_block_num\"]%12+1\n","    #then the dummy for long monthes - they have additional day;\n","    df[\"long_m_dummy\"]=(df[\"month_cat\"]==1)*1+(df[\"month_cat\"]==3)*1+(df[\"month_cat\"]==5)*1+(df[\"month_cat\"]==7)*1+(df[\"month_cat\"]==8)*1+(df[\"month_cat\"]==10)*1+(df[\"month_cat\"]==12)*1\n","    #and also dummy for December;\n","    df[\"dec_dummy\"]=(df[\"month_cat\"]==12)*1\n","    #finally check the number of unique items - that is number of unique items of current month - test data allows us; \n","    #That differes it with 'Uni_index' which is lagged;\n","    temp=df.groupby('date_block_num')['item_id'].apply(lambda x: len(x.unique())).rename(\"uni_index_now\")\n","    df[\"uni_index_now\"]=df['date_block_num'].map(temp)\n","    return df\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PRi1DfF6qUrX","colab_type":"code","colab":{}},"source":["def add_shop_feat(df,data_place,temp_shop_enc,norm=True):\n","    '''\n","    df is the dataframe where to add encodings and new features; \n","    data_place is the place where data is stored to find shops file;\n","    temp_shop_enc is initially saved shop encodings with prices and sales;\n","    '''\n","    #part 1 read the shops file and, extract the city;\n","    shops = pd.read_csv(os.path.join(data_place, 'shops-translated.csv'))\n","    #get the city names;\n","    shops['city']=shops.iloc[:,0].apply(lambda x: x.split(\" \")).apply(lambda x: x[0])\n","    #convert city name to categorical feature index;\n","    shops['city_id_cat']=shops['city'].astype('category').cat.rename_categories(range(len(shops['city'].unique())))\n","    \n","    #part 2 encode the city and shop by price and sales; \n","    #take the saved encodings and add city id;\n","    temp_shop_enc[\"city_id_cat\"]=temp_shop_enc[\"shop_id\"].map(pd.Series(shops[\"city_id_cat\"].values,index=shops[\"shop_id\"].values))\n","    #then take mean of shops' encodings as encodings of city;\n","    #it is not 100% correct but it is alike geometric mean \n","    temp2=temp_shop_enc.groupby([\"date_block_num\",\"city_id_cat\"])\n","    temp_shop_enc=pd.merge(temp_shop_enc, temp2[\"lprice_shop\"].mean().rename('lprice_city').reset_index(), on=[\"date_block_num\",'city_id_cat'], how='left')\n","    temp_shop_enc=pd.merge(temp_shop_enc, temp2[\"ltarget_shop\"].mean().rename('ltarget_city').reset_index(), on=[\"date_block_num\",'city_id_cat'], how='left')\n","    temp_shop_enc=temp_shop_enc[temp_shop_enc[\"date_block_num\"]<=df[\"date_block_num\"].max()]\n","    \n","    #part 3 merging - tricky part are new shops (not often but happens) - they will lack data; \n","    #take average of country to fill na;\n","    #taking city mean is not useful as new shops are often in new city;\n","    #so add city_id_cat seperately to ensure new shops to have city;\n","    df[\"city_id_cat\"]=df[\"shop_id\"].map(pd.Series(shops[\"city_id_cat\"].values,index=shops[\"shop_id\"].values))\n","    #not need city_id for merging encodings as each city has only one city;\n","    df=pd.merge(df, temp_shop_enc.drop(columns=[\"city_id_cat\"]), on=[\"date_block_num\",\"shop_id\"], how='left')\n","    #fillna for prices as average of country;\n","    #finally take the number of unique items equal to zero at fillna as it is more for new shops; \n","    df['uni_shop'].fillna(0,inplace=True)\n","    df['low_pr_shop'].fillna(0,inplace=True) \n","    if norm==True:\n","        #in case data is normalized then fillna for prices is just 0\n","        df['ltarget_shop'].fillna(0,inplace=True)    \n","        df['ltarget_city'].fillna(0,inplace=True)\n","    else:\n","        df['ltarget_shop'].fillna(df[\"ltarget_index\"],inplace=True)    \n","        df['ltarget_city'].fillna(df[\"ltarget_index\"],inplace=True)   \n","    if norm==True:\n","        #in case data is normalized then fillna for prices is just 0\n","        df['lprice_shop'].fillna(0,inplace=True)    \n","        df['lprice_city'].fillna(0,inplace=True)\n","    else:\n","        df['lprice_shop'].fillna(df[\"lprice_index\"],inplace=True)    \n","        df['lprice_city'].fillna(df[\"lprice_index\"],inplace=True)    \n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c8Cth--ZqUrc","colab_type":"code","colab":{}},"source":["def add_categ_test(df,data_place,temp_item_enc,norm=True):\n","    '''\n","    add category features;\n","    inputs are the data_frame, the place to look for data, and saved informaiton about encodings;\n","    also have bool to either clean the test data;\n","    output is new df with new features;\n","    '''\n","    #part 1: generating features from item_categories;\n","    item_cats = pd.read_csv(os.path.join(data_place, 'item_categories-translated.csv'))\n","    #the major categories;\n","    item_cats['major_name']=item_cats.iloc[:,0].apply(lambda x: x.lower().split(\" - \")).apply(lambda x: x[0])\n","    #that is to make id of the categories;\n","    item_cats['major_cat']=item_cats['major_name'].astype('category').cat.rename_categories(range(len(item_cats['major_name'].unique())))\n","    #also some dummies for for found minor categories in Data analysis ;\n","    item_cats['is_PS']=item_cats['item_category_name'].str.contains('(?i)ps')*1\n","    item_cats['is_XBOX']=item_cats['item_category_name'].str.contains('(?i)xbox')*1\n","    item_cats['is_1C']=item_cats['item_category_name'].str.contains('(?i)1c')*1\n","    item_cats['is_digital']=item_cats['item_category_name'].str.contains('(?i)digit')*1 | item_cats['item_category_name'].str.contains('(?i)num')*1\n","    \n","    #part 2 get the items file;\n","    items = downcast_dtypes(pd.read_csv(os.path.join(data_place, 'items-translated.csv')))\n","    #Next add all new features to items file;\n","    items=pd.merge(items, item_cats.drop(columns=[\"item_category_name\",'major_name']), on=['item_category_id'], how='left')\n","    #continue to use cat for categorical; there is always a place for cat; \n","    items.rename(columns = {'item_category_id':'cat_cat'}, inplace = True)\n","    \n","    #Part 3: add the encodings - all 4 to categories and only 2 to major categories;\n","    #first add category id to encoding file;\n","    temp_item_enc[\"cat_cat\"]=temp_item_enc[\"item_id\"].map(pd.Series(items[\"cat_cat\"].values,index=items[\"item_id\"].values))\n","    #then encode it alike with cities;\n","    temp=temp_item_enc.groupby([\"date_block_num\",\"cat_cat\"])\n","    temp_item_enc=pd.merge(temp_item_enc, temp[\"lprice_item\"].mean().rename('lprice_cat').reset_index(), on=[\"date_block_num\",'cat_cat'], how='left')\n","    temp_item_enc=pd.merge(temp_item_enc, temp[\"ltarget_item\"].mean().rename('ltarget_cat').reset_index(), on=[\"date_block_num\",'cat_cat'], how='left')\n","    temp_item_enc=pd.merge(temp_item_enc, temp[\"low_pr_item\"].mean().rename('low_pr_cat').reset_index(), on=[\"date_block_num\",'cat_cat'], how='left')\n","    #categories will also have the unique values of items but lagged\n","    #current number of items in category may be added - but hardly it will work;\n","    temp_item_enc=pd.merge(temp_item_enc, temp[\"item_id\"].apply(lambda x: len(x.unique())).rename('uni_cat').reset_index(), on=[\"date_block_num\",'cat_cat'], how='left')\n","    \n","    #then add major id to encoding file;\n","    temp_item_enc['major_cat']=temp_item_enc[\"item_id\"].map(pd.Series(items['major_cat'].values,index=items[\"item_id\"].values))\n","    #then encode it alike with shops;\n","    temp=temp_item_enc.groupby([\"date_block_num\",'major_cat'])\n","    temp_item_enc=pd.merge(temp_item_enc, temp[\"lprice_item\"].mean().rename('lprice_maj').reset_index(), on=[\"date_block_num\",'major_cat'], how='left')\n","    temp_item_enc=pd.merge(temp_item_enc, temp[\"ltarget_item\"].mean().rename('ltarget_maj').reset_index(), on=[\"date_block_num\",'major_cat'], how='left')\n","    temp_item_enc=temp_item_enc[temp_item_enc[\"date_block_num\"]<=df[\"date_block_num\"].max()]\n","    \n","    #part 4 merging to df\n","    #first add the category and category variables;\n","    df=pd.merge(df,items.drop(columns=['item_name']), on=['item_id'], how='left')\n","    #then add all the rest of variables; \n","    df=pd.merge(df,temp_item_enc.drop(columns=['cat_cat','major_cat']), on=[\"date_block_num\",'item_id'], how='left')\n","    #finally make the cleaning\n","    #fillna for prices for average and target with 0\n","    if norm==True:\n","        #if norm then the mean price will be 0 as item price was centered;\n","        df['lprice_maj'].fillna(0,inplace=True)\n","        df['lprice_cat'].fillna(0,inplace=True)\n","    else:\n","        df['lprice_maj'].fillna(df[\"lprice_index\"],inplace=True)\n","        df['lprice_cat'].fillna(df[\"lprice_index\"],inplace=True)\n","    if norm==True:\n","        #if norm then the mean price will be 0 as item price was centered;\n","        df['ltarget_maj'].fillna(0,inplace=True)\n","        df['ltarget_cat'].fillna(0,inplace=True)\n","    else:\n","        df['ltarget_maj'].fillna(df[\"ltarget_index\"],inplace=True)\n","        df['ltarget_cat'].fillna(df[\"ltarget_index\"],inplace=True)\n","           \n","    df['low_pr_cat'].fillna(0,inplace=True)\n","    df['uni_cat'].fillna(0,inplace=True)   \n","    #for item use price of category\n","    df['lprice_item'].fillna(df['lprice_cat'],inplace=True)\n","    df['ltarget_item'].fillna(df['ltarget_cat'],inplace=True)\n","    df['low_pr_item'].fillna(0,inplace=True)  \n","    \n","    #then add the date of first sale\n","    df[\"item_start\"]=df['item_id'].map(df.groupby('item_id')[\"date_block_num\"].min())\n","\n","    #normalization\n","    if norm==True:\n","        df['uni_cat']=df['uni_cat']/(df[\"uni_index\"]+000000.1)  \n","        #for item start, delete current month and divide by 12 \n","        df[\"item_start\"]=(df[\"item_start\"]-df[\"date_block_num\"])/12\n","        df['if_new']=(df[\"item_start\"]==0)*1\n","    else:\n","        df['if_new']=(df[\"item_start\"]==df[\"date_block_num\"])*1\n","\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Io6MHmfZqUrg","colab_type":"text"},"source":["### target lagging function\n"]},{"cell_type":"markdown","metadata":{"id":"xSP4rckhso-o","colab_type":"text"},"source":["\n","Lag of target is one of the most powerful features.\n","\n","**NOTE**: clip all the lagged targets so that they will show the model more correct history; but models seem to have problem with predicting 20 so add the bool if lagged target was clipped - it may help to enforce predicting 20."]},{"cell_type":"code","metadata":{"id":"xB_pvSf1qUri","colab_type":"code","colab":{}},"source":["def make_lag_target(df,lag_value_target,norm=True,clip=True,clip_max=20):\n","    '''\n","    df is the dataframe with values;\n","    lag_value_target is the list of the lags for target.\n","    '''\n","    #step 0 create temp index for each combination of month, shop and item as unique value;\n","    df['temp']=df['date_block_num']*10000000+df['shop_id']*100000+df['item_id']  \n","    #loop\n","    for month_shift in lag_value_target:\n","    #instead of merge use map for target - much faster;\n","    #make second index with shifted month;\n","        df['temp_2']=(df['date_block_num'] + month_shift)*10000000+df['shop_id']*100000+df['item_id'] \n","    #do mapping of initial index by shifted data;\n","        df['{}_lag_{}'.format('target', month_shift)]=df['temp'].map(pd.Series(df['target'].values,index=df['temp_2'].values))\n","    #NA filling with zero;\n","        df['{}_lag_{}'.format('target', month_shift)].fillna(0,inplace=True)\n","        if clip==True:\n","            df['{}_{}'.format('if_cl', month_shift)]=(df['{}_lag_{}'.format('target', month_shift)]>clip_max)*1\n","            df['{}_lag_{}'.format('target', month_shift)]=df['{}_lag_{}'.format('target', month_shift)].clip(0,clip_max)\n","        if norm==True:\n","            df['{}_lag_{}'.format('target', month_shift)]=df['{}_lag_{}'.format('target', month_shift)]/20\n","    df=df.drop(columns=['temp','temp_2'])\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NN3X-dwGqUrl","colab_type":"text"},"source":["### knn features Function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"d0BEmM32uIAY","colab_type":"text"},"source":["Ideas is:\n","- Train model on last month data to ensure no leakages;\n","- Use Knn to find the in last month the items/shops that have same lagged sales hoping that they will have alike sales in current period.\n","\n","features are:\n","- most common target value of neighbours and how many neighbors have it;\n","- weighted mean value of target weighted by distance;\n","- minimal distance neighbor and its target;\n","- median distance neighbor and its target;\n","- maximum predicted target. "]},{"cell_type":"code","metadata":{"id":"BiN_tBDfqUrn","colab_type":"code","colab":{}},"source":["def knn_feat(X,Y,X_pred,k_list=7):\n","    #model;\n","    NN = NearestNeighbors(n_neighbors=k_list, metric='minkowski', n_jobs=-1,  algorithm= 'auto')\n","    #fit model on X_lag;\n","    NN.fit(X)\n","    #making predictions;\n","    NN_output = NN.kneighbors(X_pred)\n","    #finding target of predictions;\n","    Y_pred=[]\n","    for i in range(NN_output[1].shape[0]):\n","        Y_pred+=[Y.iloc[NN_output[1][i]].values.reshape(-1).astype('int64')]\n","    Y_pred=np.vstack(Y_pred)\n","    #creating the predictions;\n","    result=[]\n","    #find the prediction by majority;\n","    temp=scipy.stats.mode(Y_pred,1)\n","    result+=[np.hstack(temp[0])/20]\n","    result+=[np.hstack(temp[1])/k_list]\n","    #weighted by distance prediction;\n","    result+=[np.sum(np.multiply(NN_output[0],Y_pred),axis=1)/20]\n","    #prediction by min distance - they are the first in distance;\n","    #predictions are already sorted so take just the first one;\n","    result+=[np.array([x[0] for x in NN_output[0]])]\n","    result+=[np.array([x[0] for x in Y_pred])/20] \n","    #prediction by median distance - they are the first in distance;\n","    result+=[np.array([x[int(np.floor(k_list/2))] for x in NN_output[0]])]\n","    result+=[np.array([x[int(np.floor(k_list/2))] for x in Y_pred])/20] \n","    #maximum predicted label;\n","    result+=[np.array([np.max(x) for x in Y_pred])/20] \n","    #returning;\n","    result=np.vstack(result)\n","    return np.transpose(result)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rgIBs-h1qUrs","colab_type":"code","colab":{}},"source":["def knn_feat_full(df,start,feat,k_list=7,norm=True,special=True,shw_time=True):\n","    '''\n","    here either take full data or the splitted one;\n","    df is dataframe, start is the month to start to generate the features.\n","    '''\n","    if special==True:\n","        temp=df[np.logical_or(df['target_lag_1']>0,df['if_new']==1)][feat].copy()\n","    else:\n","        temp=df[feat].copy()\n","    fin_res=[]\n","    #that is a basic loop for calculating the knn features\n","    #normalize by square root - that would help the distribution to be closer to normal\n","    #save all in feature fin_res\n","    for i in range(start,temp['date_block_num'].max()+1):\n","        if shw_time==True: z=time.time()\n","        X=np.sqrt(temp[temp['date_block_num'].isin([i-1])].drop(columns=['date_block_num','target'])+3/8)\n","        Y=temp[temp['date_block_num'].isin([i-1])]['target']\n","        X_pred=np.sqrt(temp[temp['date_block_num'].isin([i])].drop(columns=['date_block_num','target'])+3/8)\n","        fin_res+=[knn_feat(X,Y,X_pred,k_list=7)]\n","        if shw_time==True: print(str(i),' is done', \"time taken\", str(time.time()-z))\n","    temp=np.vstack(fin_res)\n","    #save data seperatly as they will be added to main DF later in code\n","    temp=pd.DataFrame(temp)\n","    temp.columns=['mode_knn','mode_dist_knn','mean_knn','min_knn','min_dist_knn','med_knn','med_dist_knn','max_knn']\n","    return temp"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WzE4C4LTqUrx","colab_type":"text"},"source":["### encodings lagging functions\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GFqnxf2Jv0Rq","colab_type":"text"},"source":["Lag shop and item encodings to save RAM which needs to do quite a lot of mappings.\n","\n","Normalization is done only for monthly encoding by taking the difference between months - that is features to reflect dynamics."]},{"cell_type":"code","metadata":{"id":"qRLetzTBqUrz","colab_type":"code","colab":{}},"source":["def make_lag_gen(df,add_cols_lag,temp_date_enc,norm=True):\n","    '''\n","    this function lags the general features;\n","    '''\n","    temp=temp_date_enc[[\"lprice_index\",\"ltarget_index\",\"uni_index\",\"low_pr_index\"]].copy()\n","    if norm==True:\n","        #if normalization needed, just look at the difference between periods;\n","        #uni_index is not logged so do it now for diff to make sence\n","        temp.loc[:,\"uni_index\"]=np.log(temp.loc[:,\"uni_index\"])\n","        temp=temp.diff()\n","    for month_shift in add_cols_lag:\n","        #now making the monthly shift and mapping like with target; \n","        temp[\"date_block_num\"]=temp_date_enc[\"date_block_num\"]+month_shift\n","        df['{}_lag_{}'.format(\"lprice_index\", month_shift)]=df[\"date_block_num\"].map(pd.Series(temp[\"lprice_index\"].values,index=temp[\"date_block_num\"].values)).fillna(0)\n","        df['{}_lag_{}'.format(\"ltarget_index\", month_shift)]=df[\"date_block_num\"].map(pd.Series(temp[\"ltarget_index\"].values,index=temp[\"date_block_num\"].values)).fillna(0)\n","        df['{}_lag_{}'.format(\"uni_index\", month_shift)]=df[\"date_block_num\"].map(pd.Series(temp[\"uni_index\"].values,index=temp[\"date_block_num\"].values)).fillna(0)\n","        df['{}_lag_{}'.format(\"low_pr_index\", month_shift)]=df[\"date_block_num\"].map(pd.Series(temp[\"low_pr_index\"].values,index=temp[\"date_block_num\"].values)).fillna(0)\n","    return df\n","\n","def make_lag_shop(df,add_cols_lag,temp_shop_enc):\n","    '''\n","    this function lags for the shops\n","    '''\n","    temp=temp_shop_enc[['lprice_shop', 'uni_shop', 'ltarget_shop','low_pr_shop']].copy()\n","    #no need for normalization as it was already done at encoding level\n","    df['temp']=df['date_block_num']*100+df['shop_id']    \n","    for month_shift in add_cols_lag:\n","        #now making the monthly shift and mapping;\n","        temp['temp']=(temp_shop_enc[\"date_block_num\"]+month_shift)*100+temp_shop_enc['shop_id']\n","        df['{}_lag_{}'.format('lprice_shop', month_shift)]=df[\"temp\"].map(pd.Series(temp['lprice_shop'].values,index=temp[\"temp\"].values)).fillna(0)\n","        df['{}_lag_{}'.format('uni_shop', month_shift)]=df[\"temp\"].map(pd.Series(temp['uni_shop'].values,index=temp[\"temp\"].values)).fillna(0)\n","        df['{}_lag_{}'.format('ltarget_shop', month_shift)]=df[\"temp\"].map(pd.Series(temp['ltarget_shop'].values,index=temp[\"temp\"].values)).fillna(0)\n","        df['{}_lag_{}'.format('low_pr_shop', month_shift)]=df[\"temp\"].map(pd.Series(temp['low_pr_shop'].values,index=temp[\"temp\"].values)).fillna(0)\n","        temp=temp.drop(columns=['temp'])\n","    return df\n","\n","def make_lag_item(df,add_cols_lag,temp_item_enc):\n","    ''' \n","    this function lags for the items\n","    '''\n","    temp=temp_item_enc[['lprice_item','ltarget_item','low_pr_item']].copy()\n","    #no need for normalization as it was already done at encoding level\n","    df['temp']=df['date_block_num']*100+df['item_id']    \n","    for month_shift in add_cols_lag:\n","        #now making the monthly shift and mapping\n","        temp['temp']=(temp_item_enc[\"date_block_num\"]+month_shift)+temp_item_enc['item_id']*100\n","        df['{}_lag_{}'.format('lprice_item', month_shift)]=df[\"temp\"].map(pd.Series(temp['lprice_item'].values,index=temp[\"temp\"].values)).fillna(0)\n","        df['{}_lag_{}'.format('ltarget_item', month_shift)]=df[\"temp\"].map(pd.Series(temp['ltarget_item'].values,index=temp[\"temp\"].values)).fillna(0)\n","        df['{}_lag_{}'.format('low_pr_item', month_shift)]=df[\"temp\"].map(pd.Series(temp['low_pr_item'].values,index=temp[\"temp\"].values)).fillna(0)\n","        temp=temp.drop(columns=['temp'])\n","    return df\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QMwZ3IWpqUr2","colab_type":"text"},"source":["### index-based features function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pXEtPeWfxR7o","colab_type":"text"},"source":["In this part the rank features are added; Idea is that shops are given in near linear order and items are not sorted in test data. So, items are added by rolling of unique items per shop. \n","\n","First items for first shop will have high probability of sale due to the way items are stacked; Such trend will preserve for first shops and decrease for further shops - see EDA.\n","\n","Features will be added:\n","- order of shop and item in reverse order (biggest number correspond to first) and normalize by number of shops/items that month;\n","- rolling means of target lag 1 for items for each shop; Rolling at 10 and 100 windows; Normalize by total mean.\n"]},{"cell_type":"code","metadata":{"id":"LnnET-mkqUr3","colab_type":"code","colab":{}},"source":["def index_feat(df,temp_order):\n","    '''\n","    input df as data frame that have lagged target;\n","    temp_order is needed as it has unique values of shops and items - to save time;\n","    below are working variables;\n","    '''\n","    result_sh=[]\n","    index_sh=[]\n","    result_it=[]\n","    index_it=[]\n","    res_rol_10=[]\n","    res_rol_100=[]\n","    #in this part look at the index value of shops and items relative to the month;\n","    #make it reverse order and divide by number of unique shops/items - in this case first shop/item always have value 1;\n","    for i in range(df['date_block_num'].min(),df['date_block_num'].max()+1):\n","        if i<(len(temp_order[0])):\n","            #so this part is for shops - find all unique;\n","            temp=temp_order[0][i]\n","            #make the index variable;\n","            result_sh+=[np.array(list(range(1,len(temp)+1)[::-1]))/len(temp)]\n","            #make the index for mapping of new variable;\n","            index_sh+=[temp+100*i]\n","            #so this part is for items - find all unique;\n","            temp=temp_order[1][i]\n","            #make the index variable\n","            result_it+=[np.array(list(range(1,len(temp)+1)[::-1]))/len(temp)]\n","            #make the index for mapping of new variable;\n","            index_it+=[temp*100+i]\n","            #now in this part make 2 rolling variables;\n","            #for each shop in given month calculate rolling mean;\n","            #NA for first values fill with just values;\n","            for j in temp_order[0][i]:\n","                temp2=df[df['date_block_num']==i][df['shop_id']==j]['target_lag_1']\n","                temp3=temp2.rolling(10).mean().fillna(temp2).values\n","                res_rol_10+=[temp3-np.mean(temp3)]\n","                temp3=temp2.rolling(100).mean().fillna(temp2).values\n","                res_rol_100+=[temp3-np.mean(temp3)]\n","        else:\n","            #same but for month not in temp_order;\n","            temp=df[df['date_block_num']==i]['item_id'].unique()\n","            result_it+=[np.array(list(range(1,len(temp)+1)[::-1]))/len(temp)]\n","            index_it+=[temp*100+i]            \n","            temp=df[df['date_block_num']==i]['shop_id'].unique()\n","            result_sh+=[np.array(list(range(1,len(temp)+1)[::-1]))/len(temp)]\n","            index_sh+=[temp+100*i]\n","            for j in list(temp):\n","                temp2=df[df['date_block_num']==i][df['shop_id']==j]['target_lag_1']    \n","                temp3=temp2.rolling(10).mean().fillna(temp2).values\n","                res_rol_10+=[temp3-np.mean(temp3)]\n","                temp3=temp2.rolling(100).mean().fillna(temp2).values\n","                res_rol_100+=[temp3-np.mean(temp3)]           \n","       \n","    #finally mapping of new variables;\n","    df['temp']=df['shop_id']+100*df['date_block_num']        \n","    df['shop_index']=df['temp'].map(pd.Series(np.hstack(result_sh),index=np.hstack(index_sh)))        \n","    df['temp']=df['item_id']*100+df['date_block_num']        \n","    df['item_index']=df['temp'].map(pd.Series(np.hstack(result_it),index=np.hstack(index_it)))        \n","    #adding rolling;\n","    df['roll_10']=np.hstack(res_rol_10)\n","    df['roll_100']=np.hstack(res_rol_100)\n","    \n","    return df.drop(columns=[\"temp\"])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ulxS0Pt0qUr8","colab_type":"text"},"source":["### tfidf (pca) features function\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MC0WD90rySC2","colab_type":"text"},"source":["Use Tfidf to extract valuable information from item names; extract most common words from test items names and then add as feature to all items' names; \n","\n","Tfidf is basic: clean data and then extract all words or combination that appear at least in 0.5% of test item names; \n","\n","As there is too many features, PCA will be used to shrink it to 30 features."]},{"cell_type":"code","metadata":{"id":"MHQzL5xsqUr9","colab_type":"code","colab":{}},"source":["def vectorizer(df,data_place,min_df=0.005,n_components=30):    \n","    '''\n","    the idea is to take the words used in 0.5% names of test items as paramters- that can be useful\n","    '''\n","    #load the data;\n","    items = downcast_dtypes(pd.read_csv(os.path.join(data_place, 'items-translated.csv')))\n","    df_test = downcast_dtypes(pd.read_csv(os.path.join(data_place, 'test.csv.gz')))\n","    #take the names of the items from test;\n","    items_test=df_test[\"item_id\"]\n","    items=pd.Series(items[\"item_name\"],index=items[\"item_id\"])\n","    #clean the names; \n","    items_clean=items.str.replace(r'[^\\w\\s]', ' ').str.replace(r' +', ' ').apply(lambda x: x.lower().strip())\n","    #create the vectorizer\n","    vect = TfidfVectorizer(min_df=min_df, ngram_range=(1,2),stop_words='english').fit(items_test.map(items_clean).unique())\n","    #now vectorize the items in item_id in all data train and test merged;  \n","    train_items=df['item_id'].unique()\n","    temp=vect.transform(pd.Series(train_items,index=train_items).map(items_clean)).todense()\n","    #get some sparse matrix of one-hot for each word from Vectorizer\n","    #hower it is very sparse and some words are indeed a combination of other like PC and PC Gaming\n","    #use PCA to reduce dimension but more important to seperate these features as far as possible\n","    pca = PCA(n_components = n_components).fit(temp)\n","    temp2=pca.transform(temp)\n","    print(\"PCA explained \",int(pca.explained_variance_ratio_.cumsum()[-1]*100),\"%\")\n","    temp3=pd.DataFrame(temp2,index=train_items).reset_index()\n","    temp3.columns=[\"item_id\"]+[\"pca_\"+str(x) for x in range(n_components)]\n","    df=pd.merge(df,temp3,on='item_id', how='left')\n","    return df"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r5JeGqmjqUsD","colab_type":"text"},"source":["### final function \n","\n"]},{"cell_type":"markdown","metadata":{"id":"ptFzE4Wmy5tU","colab_type":"text"},"source":["The important part is that return can be 2 dataframes - as mentioned in EDA it can be a good idea to split the data by target_lag_1 and if_new they are powerful predicting variables.\n"]},{"cell_type":"code","metadata":{"id":"6Ln34iSXqUsE","colab_type":"code","colab":{}},"source":["def gen_data(data_place,clip=True,clip_max=20,lags=[1,2,3,6,9,12],cols_lag=[1,2,3,6],split=True,min_df=0.005,n_components=30,norm=True):\n","    '''\n","    the input are: data_place is the path of data files; clip is whether clip the target to 20;\n","    lags is how many lags to take for target;\n","    cols_lag= is the lags of the encodings, split is whether split the data in 2 (based on lag_1), \n","    min_df and n_components are for shrinking the TFDF of names;\n","    norm is to normalize (bring center closer to zero) or not - need for linear and not harmful for trees; \n","    '''\n","    st = time.time()\n","    temp_time=time.time()\n","    #load data\n","    print(\"load_data\")\n","    df = downcast_dtypes(pd.read_csv(os.path.join(data_place, 'sales_train.csv.gz')))\n","    print(\"time used: \",time.time()-temp_time)\n","    #first make cleaning of shops\n","    print('cat and shops clean')\n","    df=shops_clean(df)\n","    df=cat_clean(data_place,df)   \n","    #now grouping\n","    print('group data')\n","    df,temp_order=group_data(df, clip=clip,save_str=True)\n","    print(\"time used: \",time.time()-temp_time)\n","    #save encodings\n","    print('encodings saving')\n","    temp_date_enc,temp_shop_enc,temp_item_enc=prepare_encodings(df,norm=norm)\n","    print(\"time used: \",time.time()-temp_time)\n","    #create grid for trainng\n","    print(\"get_grid\")\n","    df=grid_gen(df,temp_order,save_str=True)\n","    df = downcast_dtypes(df)\n","    print(\"time used: \",time.time()-temp_time)\n","    \n","    #now unit the test data with train data so that all features are done in uniform way\n","    #add ID\n","    print(\"add_test\")\n","    df['ID']=-1\n","    #read test\n","    df_test = downcast_dtypes(pd.read_csv(os.path.join(data_place, 'test.csv.gz')))\n","    #add date as 34 and target of 0\n","    df_test['date_block_num']=34\n","    df_test['target']=0\n","    #concat the data\n","    df=pd.concat([df,df_test],sort=False)\n","    print(\"time used: \",time.time()-temp_time)\n","    \n","    #now it is time to add encodings\n","    print(\"make encoding\") \n","    df=add_gen_feat(df,temp_date_enc,norm=norm)\n","    df=add_shop_feat(df,data_place,temp_shop_enc,norm=norm)\n","    df=add_categ_test(df,data_place,temp_item_enc,norm=norm)\n","    df = downcast_dtypes(df)\n","    print(\"time used: \",time.time()-temp_time)\n","    \n","    #now make lags\n","    print(\"make target lags\") \n","    df=make_lag_target(df,lag_value_target=lags,clip=clip,clip_max=clip_max)\n","    #here clip the target before knn\n","    df['target']=df['target'].clip(0,clip_max)\n","    print(\"time used: \",time.time()-temp_time)\n","    \n","    #next make knn features based n all target lags\n","    print(\"make knn encodings\") \n","    feat=['date_block_num','target']+list(df.columns[df.columns.str.contains('target_lag')])\n","    start=np.max(lags)\n","    k_list=7\n","    temp_knn=knn_feat_full(df,start,feat,special=True,shw_time=True)\n","    print(\"time used: \",time.time()-temp_time)\n","    \n","    #then cut the df by start month\n","    print(\"make encodings lags\") \n","    #cut the months that are not used and drop the index\n","    df=df[df['date_block_num']>=start]\n","    df=df.reset_index(drop=True)\n","    \n","    #now lags of encodings\n","    df=make_lag_gen(df,cols_lag,temp_date_enc,norm=norm)\n","    df=make_lag_shop(df,cols_lag,temp_shop_enc)\n","    df=make_lag_item(df,cols_lag,temp_item_enc)\n","    df=df.drop(columns=[\"temp\"])\n","    print(\"time used: \",time.time()-temp_time)\n","    \n","    #making index features\n","    print(\"make index feature\") \n","    df=index_feat(df,temp_order)\n","    df = downcast_dtypes(df)\n","    print(\"time used: \",time.time()-temp_time)\n","    \n","    #make PCA vectors\n","    print(\"make vectors\") \n","    df=vectorizer(df,data_place,min_df=min_df,n_components=n_components)\n","    print(\"time used: \",time.time()-temp_time)\n","    \n","    #check that everything is downcasted\n","    print('last modifications')\n","    df = downcast_dtypes(df)\n","    print('TOTAL time used: ', (time.time()-st)/60, 'minutes')\n","    if split==True:\n","        temp=np.logical_or(df['target_lag_1']>0,df['if_new']==1)\n","        df_0=df[np.logical_not(temp)].reset_index(drop=True)\n","        df_1=df[temp].reset_index(drop=True)\n","        print(\"time used: \",time.time()-temp_time)\n","        return (df_0,pd.concat([df_1,temp_knn],axis=1))\n","    else:\n","        print(\"time used: \",time.time()-temp_time)\n","    \n","        return pd.concat([df,temp_knn],axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OKnrfsVGqUsP","colab_type":"code","outputId":"9ac7a799-64f2-4251-cbc3-66157f4614ad","executionInfo":{"status":"ok","timestamp":1570750844114,"user_tz":360,"elapsed":563374,"user":{"displayName":"DAVID SUN","photoUrl":"","userId":"04535500568104874807"}},"colab":{"base_uri":"https://localhost:8080/","height":952}},"source":["df_0,df_1=gen_data(data_place)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["load_data\n","time used:  3.099313974380493\n","cat and shops clean\n","group data\n","time used:  6.660418510437012\n","encodings saving\n","time used:  7.76926064491272\n","get_grid\n","time used:  14.767264604568481\n","add_test\n","time used:  15.547861337661743\n","make encoding\n","time used:  40.45273685455322\n","make target lags\n","time used:  62.95737147331238\n","make knn encodings\n","12  is done time taken 24.813353061676025\n","13  is done time taken 18.374409198760986\n","14  is done time taken 16.336776733398438\n","15  is done time taken 16.587640047073364\n","16  is done time taken 15.676235437393188\n","17  is done time taken 13.800678253173828\n","18  is done time taken 14.87444519996643\n","19  is done time taken 15.516558170318604\n","20  is done time taken 14.539301633834839\n","21  is done time taken 16.524043798446655\n","22  is done time taken 19.03078866004944\n","23  is done time taken 22.185057401657104\n","24  is done time taken 22.181811809539795\n","25  is done time taken 13.971884727478027\n","26  is done time taken 13.244656085968018\n","27  is done time taken 13.615544080734253\n","28  is done time taken 10.355935335159302\n","29  is done time taken 10.007462739944458\n","30  is done time taken 10.601794242858887\n","31  is done time taken 10.545668363571167\n","32  is done time taken 12.716874361038208\n","33  is done time taken 13.410630464553833\n","34  is done time taken 14.05686354637146\n","time used:  416.70526242256165\n","make encodings lags\n","time used:  425.7609565258026\n","make index feature\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"],"name":"stderr"},{"output_type":"stream","text":["time used:  512.6872396469116\n","make vectors\n","PCA explained  61 %\n","time used:  528.8023335933685\n","last modifications\n","TOTAL time used:  9.004768502712249 minutes\n","time used:  544.5114378929138\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AjGKowHpVtiK","colab_type":"text"},"source":["load_data\n","cat and shops clean\n","group data\n","encodings saving\n","get_grid\n","add_test\n","make encoding\n","time used:  40.45273685455322\n","make target lags\n","time used:  62.95737147331238\n","make knn encodings\n","12  is done time taken 24.813353061676025\n","13  is done time taken 18.374409198760986\n","14  is done time taken 16.336776733398438\n","15  is done time taken 16.587640047073364\n","16  is done time taken 15.676235437393188\n","17  is done time taken 13.800678253173828\n","18  is done time taken 14.87444519996643\n","19  is done time taken 15.516558170318604\n","20  is done time taken 14.539301633834839\n","21  is done time taken 16.524043798446655\n","22  is done time taken 19.03078866004944\n","23  is done time taken 22.185057401657104\n","24  is done time taken 22.181811809539795\n","25  is done time taken 13.971884727478027\n","26  is done time taken 13.244656085968018\n","27  is done time taken 13.615544080734253\n","28  is done time taken 10.355935335159302\n","29  is done time taken 10.007462739944458\n","30  is done time taken 10.601794242858887\n","31  is done time taken 10.545668363571167\n","32  is done time taken 12.716874361038208\n","33  is done time taken 13.410630464553833\n","34  is done time taken 14.05686354637146\n","time used:  416.70526242256165\n","make encodings lags\n","time used:  425.7609565258026\n","make index feature\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:47: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n","time used:  512.6872396469116\n","make vectors\n","PCA explained  61 %\n","time used:  528.8023335933685\n","last modifications\n","TOTAL time used:  9.004768502712249 minutes\n","time used:  544.5114378929138"]},{"cell_type":"code","metadata":{"id":"2mzjXZWjqUsV","colab_type":"code","outputId":"34f6ad6a-467e-4cd0-f9e8-16741e0cf09d","executionInfo":{"status":"ok","timestamp":1570750844117,"user_tz":360,"elapsed":563370,"user":{"displayName":"DAVID SUN","photoUrl":"","userId":"04535500568104874807"}},"colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["print('shape of df_1',df_1.shape)\n","print('shape of df_0',df_0.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["shape of df_1 (1165858, 139)\n","shape of df_0 (4809172, 131)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cs-597_LqUsZ","colab_type":"text"},"source":["## save generated data \n","\n"]},{"cell_type":"markdown","metadata":{"id":"2u2B-eqr6WaO","colab_type":"text"},"source":["### save part 1:\n","Start with saving the data with lag_1>0 and if_new; as it is compressed saving then columns are to be saved separately."]},{"cell_type":"code","metadata":{"id":"5raLmLxsqUsb","colab_type":"code","colab":{}},"source":["def save_sparce_short(df,data_place,name):\n","    '''\n","    save columns and values separatly\n","    '''\n","    temp_col=df.columns.values\n","    #columns to be saved in pickle\n","    with open(os.path.join(data_place,'generated_data/', str(name+'_cols.pickle')), 'wb') as file:\n","        pickle.dump(temp_col, file)    \n","    #saving the file\n","    name_train=name+'.gz'\n","    df.to_csv(os.path.join(data_place,'generated_data/', name_train), compression='gzip',index=False,header=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RpEn5vKTqUsh","colab_type":"code","colab":{}},"source":["save_sparce_short(df_1,data_place,'data1')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fJBf869_qUsm","colab_type":"text"},"source":["### save part 2\n"]},{"cell_type":"markdown","metadata":{"id":"PE95lrWi7VTR","colab_type":"text"},"source":["\n","The data with lag_1==0 is much larger and have 5 mln data records. This part is very slow and RAM intensive."]},{"cell_type":"code","metadata":{"id":"UOOrVfZCqUst","colab_type":"code","colab":{}},"source":["def save_sparce_long(df,data_place,name):\n","    #save columns\n","    temp_col=df.columns.values\n","    #columns to be saved in pickle\n","    with open(os.path.join(data_place,'generated_data/', str(name+'_cols.pickle')), 'wb') as file:\n","        pickle.dump(temp_col, file)\n","    #split the data on train and rest\n","    #save train data\n","    name_train=name+'_train.gz'\n","    #now save compressed file \n","    df[df['date_block_num']<30].to_csv(os.path.join(data_place,'generated_data/', name_train), compression='gzip',index=False,header=False)\n","    #save second part\n","    name_train=name+'_ens.gz'\n","    df[df['date_block_num']>29].to_csv(os.path.join(data_place,'generated_data/', name_train), compression='gzip',index=False,header=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"p4LrnuVIqUs0","colab_type":"code","colab":{}},"source":["save_sparce_long(df_0,data_place,'data0')"],"execution_count":0,"outputs":[]}]}